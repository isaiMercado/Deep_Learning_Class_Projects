{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './Libs')\n",
    "from freeze_graph import freeze_graph\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import scipy\n",
    "import scipy.misc\n",
    "from scipy import ndimage\n",
    "from scipy.misc import toimage\n",
    "from enum import Enum\n",
    "from sklearn.utils import shuffle\n",
    "import ntpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 60\n",
    "LEARNING_RATE = 0.0001\n",
    "STEPS_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: './/features.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-4a642bf7e9d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mloaded_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_matrix_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mloaded_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_matrix_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4a642bf7e9d7>\u001b[0m in \u001b[0;36mload_matrix_from_disk\u001b[1;34m(folder, name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_matrix_from_disk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\".npy\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/isai/anaconda2/lib/python2.7/site-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: [Errno 2] No such file or directory: './/features.npy'"
     ]
    }
   ],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "        \n",
    "    def get_next_batch(self, batch_size):\n",
    "        self.features, self.labels = shuffle(self.features, self.labels)\n",
    "        return self.features[0:batch_size,:], self.labels[0:batch_size,:]\n",
    "        \n",
    "        \n",
    "class Data(object):\n",
    "    def __init__(self, features, labels):\n",
    "        \n",
    "        partition = features.shape[0] / 4\n",
    "        \n",
    "        train_features = features[0:partition*3,:]\n",
    "        train_labels = labels[0:partition*3,:]\n",
    "        \n",
    "        test_features = features[partition*3:partition*4,:]\n",
    "        test_labels = labels[partition*3:partition*4,:]\n",
    "        \n",
    "        self.train = Dataset(train_features, train_labels)\n",
    "        self.test = Dataset(test_features, test_labels)\n",
    "        \n",
    "\n",
    "def load_matrix_from_disk(folder, name):\n",
    "    matrix = np.load(folder + \"/\" + name + \".npy\")\n",
    "    return matrix\n",
    "\n",
    "loaded_features = load_matrix_from_disk(\"./\", \"features\")\n",
    "loaded_labels = load_matrix_from_disk(\"./\", \"labels\")\n",
    "\n",
    "data = Data(loaded_features, loaded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = data.test.get_next_batch(1)\n",
    "plt.imshow(a.reshape((64,64,3)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x and y coordinates to plot \n",
    "epochs = EPOCHS / STEPS_SIZE\n",
    "x_coordinates = np.linspace(0, epochs, num=epochs)\n",
    "loss_y_coordinates = np.zeros(epochs)\n",
    "accuracy_y_coordinates = np.zeros(epochs)\n",
    "\n",
    "def plot(x, y, limits, title, x_label_name, y_label_name):\n",
    "    plt.close(\"all\")\n",
    "    figure = plt.figure()\n",
    "    figure.clf()\n",
    "    \n",
    "    plt.plot(x, y)\n",
    "    \n",
    "    #plt.axis(limits)\n",
    "    plt.ylabel(y_label_name)\n",
    "    plt.xlabel(x_label_name)\n",
    "\n",
    "    figure.suptitle(title, fontsize=20)\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_loss():\n",
    "    limits = [0,EPOCHS,0,5]\n",
    "    plot(x_coordinates, loss_y_coordinates, limits, \"Training Loss Overtime\", \"Epochs\", \"Loss\")  \n",
    "    return\n",
    "\n",
    "def plot_accuracy():\n",
    "    limits = [0,EPOCHS,0,1]\n",
    "    plot(x_coordinates, accuracy_y_coordinates, limits, \"Test Set Accuracy Overtime\", \"Epochs\", \"Accuracy\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining some functions that make the defining graph code look cleaner\n",
    "\n",
    "def weight_variable(shape,name):\n",
    "    gaussian_matrix = tf.truncated_normal(shape, stddev=0.1)\n",
    "    weight_matrix = tf.Variable(gaussian_matrix, name=name)\n",
    "    #weight_matrix = tf.get_variable(initializer=tf.contrib.layers.xavier_initializer(), shape=shape, name=name)\n",
    "    return weight_matrix\n",
    "\n",
    "def bias_variable(shape,name):\n",
    "    gaussian_vector = tf.truncated_normal(shape, stddev=0.1)\n",
    "    bias_vector = tf.Variable(gaussian_vector, name=name)\n",
    "    #bias_vector = tf.get_variable(initializer=tf.contrib.layers.xavier_initializer(), shape=shape, name=name)\n",
    "    return bias_vector\n",
    "\n",
    "def convolution(images_matrix, weight_matrix):\n",
    "    # [batch, height, width, depth]\n",
    "    strides = [1,1,1,1]\n",
    "    convoluted_images_matrix = tf.nn.conv2d(images_matrix, weight_matrix, strides=strides, padding='SAME')\n",
    "    return convoluted_images_matrix\n",
    "\n",
    "def max_pool_2x2(images_matrix):\n",
    "    # [batch, height, width, depth]\n",
    "    strides = [1,2,2,1]\n",
    "    ksize = [1,2,2,1] #[1,2,2,1]\n",
    "    smaller_images_matrix = tf.nn.max_pool(images_matrix, ksize=ksize, strides=strides, padding='SAME')\n",
    "    return smaller_images_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining the graph (In this case it is convolutional neural network)\n",
    "\n",
    "IMAGE_SIDE = 28 #64\n",
    "DEPTH = 1 #3\n",
    "CLASSES = 10 #4\n",
    "\n",
    "NEURONS = 32\n",
    "\n",
    "# inputs\n",
    "targets_matrix = tf.placeholder(tf.float32, shape=[None, CLASSES], name=\"targets_matrix\")\n",
    "features_matrix = tf.placeholder(tf.float32, shape=[None, IMAGE_SIDE * IMAGE_SIDE * DEPTH], name='features_matrix') # 3072 784\n",
    "\n",
    "# reshaping images as grids instead of vectors for convolution\n",
    "# [rows, image_width, image_height, image_depth]\n",
    "images_matrix = tf.reshape(features_matrix, [-1, IMAGE_SIDE, IMAGE_SIDE, DEPTH], name=\"input_node\") # -1, 28, 28, 1\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('convolutional1') as scope:\n",
    "    # hidden inputs\n",
    "    # [filter_width, filter_height, input_depth, num_filters]\n",
    "    weight_matrix_conv1 = weight_variable([5, 5, DEPTH, NEURONS],name=\"weight_matrix_conv1\")\n",
    "    # one bias per filter [num_filters]\n",
    "    bias_vector_conv1 = bias_variable([NEURONS], name=\"bias_vector_conv1\")\n",
    "\n",
    "    # linear operation\n",
    "    linear_convoluted_matrix_conv1 = convolution(images_matrix, weight_matrix_conv1) + bias_vector_conv1\n",
    "\n",
    "    # nonlinear operation\n",
    "    nonlinear_convoluted_matrix_conv1 = tf.nn.relu(linear_convoluted_matrix_conv1)\n",
    "\n",
    "    # making output smaller\n",
    "    smaller_matrix_conv1 = max_pool_2x2(nonlinear_convoluted_matrix_conv1)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('convolutional2') as scope:\n",
    "    # hidden inputs\n",
    "    # [filter_width, filter_height, input_depth, num_filters]\n",
    "    weight_matrix_conv2 = weight_variable([5, 5, NEURONS, NEURONS * 2], name=\"weight_matrix_conv2\")\n",
    "    # one bias per filter [num_filters]\n",
    "    bias_vector_conv2 = bias_variable([NEURONS * 2], name=\"bias_vector_conv2\")\n",
    "\n",
    "    # linear operation\n",
    "    linear_convoluted_matrix_conv2 = convolution(smaller_matrix_conv1, weight_matrix_conv2) + bias_vector_conv2\n",
    "\n",
    "    # nonlinear operation\n",
    "    nonlinear_convoluted_matrix_conv2 = tf.nn.relu(linear_convoluted_matrix_conv2)\n",
    "\n",
    "    # making output smaller\n",
    "    smaller_matrix_conv2 = max_pool_2x2(nonlinear_convoluted_matrix_conv2)\n",
    "\n",
    "    # making output flat for fully connected layer\n",
    "    # [rows, input_width * input_height * input_depth]\n",
    "    smaller_matrix_conv2_flat = tf.reshape(smaller_matrix_conv2, [-1, (IMAGE_SIDE / 4) * (IMAGE_SIDE / 4) * (NEURONS * 2)]) #[-1, 7*7*64]\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('fully_connected_hidden1') as scope:\n",
    "    # hidden inputs\n",
    "    # [input_width * input_height * input_depth, num_neuros]\n",
    "    weight_matrix_fc1 = weight_variable([(IMAGE_SIDE / 4) * (IMAGE_SIDE / 4) * (NEURONS * 2), 2048], name=\"weight_matrix_fc1\")\n",
    "    # one bias per neuron [num_neurons]\n",
    "    bias_vector_fc1 = bias_variable([2048], name=\"bias_vector_fc1\")\n",
    "\n",
    "    # linear operation\n",
    "    linear_hidden_matrix_fc1 = tf.matmul(smaller_matrix_conv2_flat, weight_matrix_fc1) + bias_vector_fc1\n",
    "\n",
    "    # nonlinear operation\n",
    "    nonlinear_hidden_matrix_fc1 = tf.nn.relu(linear_hidden_matrix_fc1)\n",
    "\n",
    "\n",
    "\n",
    "with tf.name_scope('fully_connected_output') as scope:\n",
    "    # hidden inputs\n",
    "    # [num_neurons, classes]\n",
    "    weight_matrix_fc2 = weight_variable([2048, CLASSES], name=\"weight_matrix_fc2\")\n",
    "    # one bias per class [classes]\n",
    "    bias_vector_fc2 = bias_variable([CLASSES], name=\"bias_vector_fc2\")\n",
    "\n",
    "    # linear operation\n",
    "    output_matrix_fc2 = tf.matmul(nonlinear_hidden_matrix_fc1, weight_matrix_fc2) + bias_vector_fc2\n",
    "\n",
    "    # making output probabilities\n",
    "    probabilities_matrix = tf.nn.softmax(output_matrix_fc2, name = \"output_node\")\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(targets_matrix * tf.log(probabilities_matrix), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(probabilities_matrix,1), tf.argmax(targets_matrix,1))\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "summaries = tf.merge_all_summaries()\n",
    "summary_writer = tf.train.SummaryWriter('./TensorBoard',graph=sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initializing session and graph\n",
    "\n",
    "\n",
    "\n",
    "# Training the convolutional neural network\n",
    "step = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # This is my data\n",
    "    #features, labels = data.train.get_next_batch(10)\n",
    "    \n",
    "    # This is mnist data\n",
    "    batch = mnist.train.next_batch(40)\n",
    "    features, labels = batch[0], batch[1]\n",
    "    \n",
    "    sess.run(train_step, feed_dict={features_matrix: features, targets_matrix: labels})\n",
    "    \n",
    "    if epoch % STEPS_SIZE == 0:\n",
    "        l_scalar = sess.run(cross_entropy, feed_dict={features_matrix:features, targets_matrix: labels})\n",
    "        a_scalar = sess.run(accuracy, feed_dict={features_matrix:features, targets_matrix: labels})\n",
    "        loss_y_coordinates[step] = l_scalar\n",
    "        accuracy_y_coordinates[step] = a_scalar\n",
    "        step = step + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mean_accuracy = np.mean(accuracy_y_coordinates)\n",
    "print(\"Test Set Mean Accuracy\")\n",
    "print(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"./TrainedModel/\"\n",
    "checkpoint_weights_filename = \"weights\"\n",
    "checkpoint_graph_filename = \"graph.pb\"\n",
    "trained_graph_filename = \"trained_graph.pb\"\n",
    "\n",
    "checkpoint_weights_path = path + checkpoint_weights_filename\n",
    "checkpoint_graph_path = path + checkpoint_graph_filename\n",
    "trained_graph_path = path + trained_graph_filename\n",
    "saver_path = \"\"\n",
    "\n",
    "as_text = True\n",
    "as_binary = not as_text\n",
    "\n",
    "# Saving learned weights of the model\n",
    "tf.train.Saver().save(sess, checkpoint_weights_path) #, global_step=0, latest_filename=\"checkpoint_name\")\n",
    "\n",
    "# Saving graph definition\n",
    "tf.train.write_graph(sess.graph.as_graph_def(), path, checkpoint_graph_filename, as_text)\n",
    "\n",
    "# Merging graph definition and learned weights into a trained graph\n",
    "input_saver_path = \"\"\n",
    "input_binary = False\n",
    "output_node_names = \"output_node\"\n",
    "restore_op_name = \"save/restore_all\"\n",
    "filename_tensor_name = \"save/Const:0\"\n",
    "clear_devices = False\n",
    "\n",
    "freeze_graph (\n",
    "    input_graph = checkpoint_graph_path,\n",
    "    input_saver = saver_path,\n",
    "    input_checkpoint = checkpoint_weights_path,\n",
    "    output_graph = trained_graph_path,\n",
    "    \n",
    "    initializer_nodes = None, #\"input_node\",\n",
    "    output_node_names = \"fully_connected_output/output_node\",\n",
    "    \n",
    "    restore_op_name = \"save/restore_all\",\n",
    "    filename_tensor_name = \"save/Const:0\",\n",
    "    \n",
    "    input_binary = as_binary,\n",
    "    clear_devices = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
